{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution - Pogány László"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution installs the external libraries, downloads the required historical data automatically and computes the relevant metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version\n",
    "# Python 3.5.2 :: Anaconda 4.2.0 (64-bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __pandas__ library is used for storing the data in dataframes.\n",
    "<br />\n",
    "The __beautifulsoup4__ library is used for scrapping data from web.\n",
    "<br />\n",
    "The __pandas-datareader__ library is used for downloading __Yahoo Finance__ and __Google Finance__ datasets.<br />\n",
    "Further information: https://github.com/pydata/pandas-datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install scipy\n",
    "!pip install pandas\n",
    "!pip install pandas-datareader\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloader and preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __Utils__ and __InputPreprocessor__ classes are implementing common functionalities for data manipulation. They define some static helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import doctest\n",
    "doctest.testmod()\n",
    "\n",
    "\n",
    "class Utils(object):\n",
    "    \"\"\" The class contains a collection of useful functions in relation to minor data manipulations \"\"\"\n",
    "       \n",
    "    @staticmethod\n",
    "    def convertDatetimeStrFormat(dtString, fromPattern, toPattern):\n",
    "        \"\"\" The method converts a datetime object represented in formatted string to another formatted string \n",
    "        \n",
    "        >>> Utils.convertDatetimeStrFormat('1987-08-14', \"%Y-%m-%d\", \"%d/%m/%Y\")\n",
    "        '14/08/1987'\n",
    "        >>> Utils.convertDatetimeStrFormat('5/1/1990', \"%d/%m/%Y\", \"%Y-%m-%d\")\n",
    "        '1990-01-05'\n",
    "        \"\"\"\n",
    "        \n",
    "        return datetime.strptime(dtString, fromPattern).strftime(toPattern)\n",
    "\n",
    "\n",
    "\n",
    "class InputPreprocessor(object):\n",
    "    \"\"\" The class contains a collection of functions in relation to preprocessing of the historical data \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloading process of the datasets is implemented by using a __DownloadAdapter__ instance.\n",
    "<br />\n",
    "The __YahooFinanceDownloadAdapter__ and __GoogleFinanceDownloadAdapter__ classes are implementing download datasource specific solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import io\n",
    "import doctest\n",
    "doctest.testmod()\n",
    "\n",
    "\n",
    "class DownloadAdapter(ABC):\n",
    "    \"\"\" Base class for download adapters\n",
    "    \n",
    "    The subclasses are inherit the 'downloaderFunction' abstract method.\n",
    "    The abstract method's implementation will specify the real download method.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def downloaderFunction(self, symbolCode, startDateTime, endDateTime, datasource = None, other = None):\n",
    "        \"\"\" This method will be called by the data manager \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "class YahooFinanceDownloadAdapter(DownloadAdapter):\n",
    "    \"\"\" Specifies Yahoo Finance downloading functionalities by implementing DownloadAdapter \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    \n",
    "    def downloaderFunction(self, symbolCode, startDateTime, endDateTime, datasource, other):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "class GoogleFinanceDownloadAdapter(DownloadAdapter):\n",
    "    \"\"\" Specifies Google Finance downloading functionalities by implementing DownloadAdapter \n",
    "    \n",
    "    This method implements an URL based solution. The reason of not using 'pandas_datareader' library here is that\n",
    "    it seems not to work on older data. According to the experiments a templated direct URL can dowload much more\n",
    "    historical data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    \n",
    "    DOWNLOAD_URL_PATTERN = \"http://finance.google.ca/finance/historical?q=%s:%s&startdate=%s&enddate=%s&output=csv\"\n",
    "    DATE_QUERY_PATTERN   = \"%b+%d+%Y\"\n",
    "    \n",
    "    \n",
    "    def downloaderFunction(self, symbolCode, startDateTime, endDateTime, datasource, other):\n",
    "        \"\"\" This method downloads the historical data from Google Finance\n",
    "        \n",
    "        >>> gfda = GoogleFinanceDownloadAdapter()\n",
    "        >>> resultSet = gfda.downloaderFunction(\"GOOGL\", datetime(2016,1,1), datetime(2017,1,1), None, \"NASDAQ\")\n",
    "        >>> len(resultSet)\n",
    "        252\n",
    "        >>> resultSet = gfda.downloaderFunction(\"GOOGL\", datetime(1990,1,1), datetime(2017,1,1), None, \"NASDAQ\")\n",
    "        >>> len(resultSet)\n",
    "        3114\n",
    "        \"\"\"\n",
    "        \n",
    "        def checkDatetimeParam(dt, paramName):\n",
    "            assert (type(dt) is datetime), \\\n",
    "                paramName + \" parameter is not datetime, it is a(n) %s\" % type(dt)\n",
    "                \n",
    "        # checking parameter type\n",
    "        checkDatetimeParam(startDateTime, 'startDateTime')\n",
    "        checkDatetimeParam(endDateTime, 'endDateTime')\n",
    "        \n",
    "        # converting datetime into query format\n",
    "        startDateTime = startDateTime.strftime(self.DATE_QUERY_PATTERN)\n",
    "        endDateTime   = endDateTime.strftime(self.DATE_QUERY_PATTERN)\n",
    "        \n",
    "        # downloading data by using the templated URL\n",
    "        csvUrl = self.DOWNLOAD_URL_PATTERN % (other, symbolCode, startDateTime, endDateTime)\n",
    "        with urllib.request.urlopen(csvUrl) as response:\n",
    "            byteContent = response.read().decode(\"utf-8\")\n",
    "            \n",
    "            # reading data into pandas dataframe\n",
    "            data = pd.read_csv(io.StringIO(byteContent), sep=',')\n",
    "            \n",
    "            # identifying the name of the first column\n",
    "            keyColumnName = data.columns[0]    # According to the downloaded CSV this sould be a column named 'Date'\n",
    "            \n",
    "            # formatting Date column\n",
    "            def changeDateStr(string):\n",
    "                dt = datetime.strptime(string, '%d-%b-%y')\n",
    "                return dt\n",
    "            data[keyColumnName] = data[keyColumnName].apply(changeDateStr)\n",
    "                        \n",
    "            # use Date column as key\n",
    "            data.set_index(keyColumnName, inplace=True)\n",
    "            \n",
    "            return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global function __getSP400ListFromWikipedia()__ searches in the web for current list of SP400 companies.\n",
    "\n",
    "The reason of using a webscrapper is that the symbols of the SP400 are changing in time and the script always needs the current values.\n",
    "<br />\n",
    "The current values may be found here: <https://en.wikipedia.org/wiki/List_of_S%26P_400_companies>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a fresh list of 400 companies (changes by time, needs to be scraped at runtime)\n",
    "URL_WIKIPEDIA_SP400 = \"https://en.wikipedia.org/wiki/List_of_S%26P_400_companies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import doctest\n",
    "doctest.testmod()\n",
    "\n",
    "\n",
    "def getSP400ListFromWikipedia():\n",
    "    \"\"\" Returns current SP400 symbol codes from Wikipedia\n",
    "    \n",
    "    Downloads HTML content, selects the first table and returns the values from the first column of the selected table.\n",
    "    \n",
    "    >>> len(getSP400ListFromWikipedia())\n",
    "    400\n",
    "    \"\"\"\n",
    "    \n",
    "    TABLE_NUMBER = 0   # the first table's content should be downloaded from wikipedia\n",
    "    \n",
    "    with urllib.request.urlopen(URL_WIKIPEDIA_SP400) as response:\n",
    "        \n",
    "        # downloading html content\n",
    "        html = response.read()\n",
    "        \n",
    "        # parsing html content\n",
    "        htmlSoup = BeautifulSoup(html, 'lxml')\n",
    "        if htmlSoup is None:\n",
    "            return None\n",
    "        \n",
    "        # finding table in html code\n",
    "        tableHtml = htmlSoup.findAll('table', class_='wikitable sortable')[TABLE_NUMBER]\n",
    "        tableSoup = BeautifulSoup(str(tableHtml), 'lxml')\n",
    "        \n",
    "        # filtering relevant values from table\n",
    "        return [x.td.a.text for x in tableSoup('tr') if x.td]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An instance of the __DataManager__ class can download historical data by using the __DownloadAdapter__'s method.\n",
    "<br />\n",
    "The downloaded historical data are loaded into pandas dataframes.\n",
    "The pandas dataframes are saved in form of CSV files and SQL data tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import doctest\n",
    "doctest.testmod()\n",
    "\n",
    "\n",
    "class DataManager:\n",
    "    \"\"\" Downloads and manages historical OHLC and volume data\n",
    "    \n",
    "    Downloads each record exactly only once\n",
    "    Stores the downloaded history as CSV files locally\n",
    "    \n",
    "    TODO: Stores the downloaded history in SQL database locally\n",
    "    TODO: Makes queries in the database after already stored data, if data is not present downloads it\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas_datareader as pdr\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def convertDateTimeToString(self, dt, datetime_pattern = \"%Y%m%d\"):\n",
    "        \"\"\" Converter for datetime types\n",
    "        \n",
    "        In case of datetime parameter returns a formatted datetime string, otherwise returns the original\n",
    "        \n",
    "        >>> x = DataManager()\n",
    "        >>> x.convertDateTimeToString(datetime(2016,1,2,3,4,5))\n",
    "        '20160102'\n",
    "        >>> x.convertDateTimeToString(datetime(2016,1,2,3,4,5), \"%Y%m%d-%H%M%S\")\n",
    "        '20160102-030405'\n",
    "        >>> x.convertDateTimeToString('2016-01-01')\n",
    "        '2016-01-01'\n",
    "        >>> x.convertDateTimeToString(2016)\n",
    "        Traceback (most recent call last):\n",
    "          ...\n",
    "        AssertionError: dt parameter is not string nor datetime, it is a(n) <class 'int'>\n",
    "        \"\"\"\n",
    "        \n",
    "        # checking parameter type\n",
    "        assert (type(dt) is datetime or type(dt) is str), \\\n",
    "            \"dt parameter is not string nor datetime, it is a(n) %s\" % type(dt)\n",
    "        \n",
    "        # converversion with formatting datetime to string if needed\n",
    "        dtStr = dt\n",
    "        if(type(dt) is datetime):\n",
    "            dtStr = dt.strftime(datetime_pattern)\n",
    "            \n",
    "        return dtStr\n",
    "    \n",
    "    \n",
    "    def generateCsvFileName(self, dataSource, symbolCode, startDateTime, endDateTime, \n",
    "                            filename_pattern = \"%s_%s_start=%s_end=%s.download.csv\"):\n",
    "        \"\"\" CSV file name generator\n",
    "        \n",
    "        Uses the sysmbol code, a starting and an ending date to generate a filename.\n",
    "        \n",
    "        >>> x = DataManager()\n",
    "        >>> x.generateCsvFileName(\"google\", \"AAPL\", \"2016-01-01\", \"2017-01-01\")\n",
    "        'google_AAPL_start=2016-01-01_end=2017-01-01.download.csv'\n",
    "        >>> x.generateCsvFileName(\"google\", \"AAPL\", \"2016-01-01\", \"2017-01-01\", \"download.%s.%s.%s.%s.csv\")\n",
    "        'download.google.AAPL.2016-01-01.2017-01-01.csv'\n",
    "        >>> x.generateCsvFileName(\"google\", \"AAPL\", datetime(2016,1,1), datetime(2017,1,1))\n",
    "        'google_AAPL_start=20160101_end=20170101.download.csv'\n",
    "        >>> x.generateCsvFileName(1000, \"AAPL\", \"2016-01-01\", \"2017-01-01\")\n",
    "        Traceback (most recent call last):\n",
    "          ...\n",
    "        AssertionError: dataSource parameter is not string, it is a(n) <class 'int'>\n",
    "        \"\"\"\n",
    "        \n",
    "        # checking parameter type\n",
    "        assert (type(dataSource) is str), \\\n",
    "            \"dataSource parameter is not string, it is a(n) %s\" % type(dataSource)\n",
    "        \n",
    "        # converting dates to string\n",
    "        startDateTimeStr = self.convertDateTimeToString(startDateTime)\n",
    "        endDateTimeStr   = self.convertDateTimeToString(endDateTime)\n",
    "        \n",
    "        return filename_pattern % (dataSource, symbolCode, startDateTimeStr, endDateTimeStr)\n",
    "        \n",
    "    \n",
    "    def downloadData(self, downloaderFunction, datasource=None):\n",
    "        \"\"\" Returns a method with curried parameters which can download historical data \"\"\"\n",
    "        \n",
    "        def downloader(symbolCode, startDateTime, endDateTime):\n",
    "            \"\"\" Returns a closure with fixed downloaderFunction and datasource \"\"\"\n",
    "            return downloaderFunction(symbolCode, startDateTime, endDateTime, datasource)\n",
    "        \n",
    "        return downloader\n",
    "    \n",
    "    \n",
    "    def saveDataframeToCsv(self, symbol, dataframe, startDateTime=None, endDateTime=None):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def getData(self, symbolOrSymbolList, startDateTime, endDateTime):\n",
    "        pass\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interval border constants\n",
    "DATA_MEAN_START     = datetime(2016,1,1)\n",
    "DATA_MEAN_END       = datetime(2017,1,1)\n",
    "DATA_VARIANCE_START = datetime(2016,2,11)\n",
    "DATA_VARIANCE_END   = datetime(2016,11,8)\n",
    "DATA_MINMAX_START   = datetime(2016,1,18)    # including\n",
    "DATA_MINMAX_END     = datetime(2016,10,18)   # excluding\n",
    "DATA_STD_START      = datetime(2016,4,17)\n",
    "DATA_STD_END        = datetime(2016,12,15)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
