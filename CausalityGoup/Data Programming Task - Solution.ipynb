{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution - Pogány László"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason of selecting __Jupyter/IPython Notebook__ against a simple Python project is the self documentation support. \n",
    "Although the solutions implemented in this way can't be integrated directly into the existing operative systems (without further work), but it is much easier to demonstrate the whole process and the implemented functionalities step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution installs the external libraries, downloads the required historical data automatically and computes the relevant metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version\n",
    "# Python 3.5.2 :: Anaconda 4.2.0 (64-bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __pandas__ library is used for storing the data in dataframes.\n",
    "<br />\n",
    "The __beautifulsoup4__ library is used for scrapping data from web.\n",
    "<br />\n",
    "The __pandas-datareader__ library is used for downloading __Yahoo Finance__ and __Google Finance__ datasets.<br />\n",
    "Further information: https://github.com/pydata/pandas-datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datetime\n",
    "!pip install bs4\n",
    "!pip install requests\n",
    "!pip install numpy\n",
    "!pip install scipy\n",
    "!pip install pandas\n",
    "!pip install pandas-datareader\n",
    "!pip install intervaltree\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloader and preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predefined directory names for the application.\n",
    "The downloads and the calculated outputs are placed into special directories inside the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# constants and directory structure for the application\n",
    "OUTPUT_DIRECTORY_PATH   = r'..\\out'\n",
    "LOG_DIRECTORY_PATH      = os.path.join(OUTPUT_DIRECTORY_PATH, \"logs\")\n",
    "DOWNLOAD_DIRECTORY_NAME = os.path.join(OUTPUT_DIRECTORY_PATH, \"data\")\n",
    "DATABASE_DIRECTORY_NAME = os.path.join(OUTPUT_DIRECTORY_PATH, \"meta\")\n",
    "\n",
    "DATABASE_FILE_NAME      = 'metadata.db'\n",
    "\n",
    "def createDirectory(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "createDirectory(OUTPUT_DIRECTORY_PATH)\n",
    "createDirectory(LOG_DIRECTORY_PATH)\n",
    "createDirectory(DOWNLOAD_DIRECTORY_NAME)\n",
    "createDirectory(DATABASE_DIRECTORY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __PlatformLogger__ function provides logging services for the modules/classes. <br />\n",
    "The __Utils__ and __InputPreprocessor__ classes are implementing common functionalities for data manipulation in form of static helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import doctest\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "doctest.testmod()\n",
    "\n",
    "\n",
    "loggers = {}\n",
    "def PlatformLogger(moduleName):\n",
    "    \"\"\" Provides logging functionalities.\n",
    "    \n",
    "    The class configures the log service, the log handlers and returns a logger object.\n",
    "    \"\"\"\n",
    "    \n",
    "    global loggers\n",
    "\n",
    "    if loggers.get(moduleName):\n",
    "        \n",
    "        return loggers.get(moduleName)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # create logger with moduleName parameter\n",
    "        logger = logging.getLogger(moduleName)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "\n",
    "        # create console handler with a higher log level\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(logging.INFO)\n",
    "\n",
    "        # create file handler which logs even debug messages\n",
    "        fh = logging.FileHandler(os.path.join(LOG_DIRECTORY_PATH, 'error.log'))\n",
    "        fr = logging.handlers.RotatingFileHandler(os.path.join(LOG_DIRECTORY_PATH, 'event.log'))\n",
    "        fh.setLevel(logging.WARN)\n",
    "        fr.setLevel(logging.DEBUG)\n",
    "\n",
    "        # create formatter and add it to the handlers\n",
    "        formatter = logging.Formatter('[%(asctime)s] - %(name)s - %(levelname)s - %(message)s')\n",
    "        fh.setFormatter(formatter)\n",
    "        fr.setFormatter(formatter)\n",
    "        ch.setFormatter(formatter)\n",
    "\n",
    "        # add the handlers to the logger\n",
    "        logger.addHandler(fh)\n",
    "        logger.addHandler(fr)\n",
    "        logger.addHandler(ch)\n",
    "        \n",
    "        # updating loggers\n",
    "        loggers.update(dict(name=logger))\n",
    "\n",
    "        return logger\n",
    "\n",
    "\n",
    "\n",
    "class Utils(object):\n",
    "    \"\"\" The class contains a collection of useful functions in relation to minor data manipulations \"\"\"\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def checkDatetimeParam(dt, paramName):\n",
    "        assert (type(dt) is datetime), \\\n",
    "            paramName + \" parameter is not datetime, it is a(n) %s\" % type(dt)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def convertDatetimeStrFormat(dtString, fromPattern, toPattern):\n",
    "        \"\"\" The method converts a datetime object represented in formatted string to another formatted string \n",
    "        \n",
    "        >>> Utils.convertDatetimeStrFormat('1987-08-14', \"%Y-%m-%d\", \"%d/%m/%Y\")\n",
    "        '14/08/1987'\n",
    "        >>> Utils.convertDatetimeStrFormat('5/1/1990', \"%d/%m/%Y\", \"%Y-%m-%d\")\n",
    "        '1990-01-05'\n",
    "        \"\"\"\n",
    "        \n",
    "        return datetime.strptime(dtString, fromPattern).strftime(toPattern)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def convertDateTimeToString(dt, datetime_pattern = \"%Y%m%d\"):\n",
    "        \"\"\" Converter for datetime types\n",
    "        \n",
    "        In case of datetime parameter returns a formatted datetime string, otherwise returns the original\n",
    "        \n",
    "        >>> Utils.convertDateTimeToString(datetime(2016,1,2,3,4,5))\n",
    "        '20160102'\n",
    "        >>> Utils.convertDateTimeToString(datetime(2016,1,2,3,4,5), \"%Y%m%d-%H%M%S\")\n",
    "        '20160102-030405'\n",
    "        >>> Utils.convertDateTimeToString('2016-01-01')\n",
    "        '2016-01-01'\n",
    "        >>> Utils.convertDateTimeToString(2016)\n",
    "        Traceback (most recent call last):\n",
    "          ...\n",
    "        AssertionError: dt parameter is not string nor datetime, it is a(n) <class 'int'>\n",
    "        \"\"\"\n",
    "        \n",
    "        # checking parameter type\n",
    "        assert (type(dt) is datetime or type(dt) is str), \\\n",
    "            \"dt parameter is not string nor datetime, it is a(n) %s\" % type(dt)\n",
    "        \n",
    "        # converversion with formatting datetime to string if needed\n",
    "        dtStr = dt\n",
    "        if(type(dt) is datetime):\n",
    "            dtStr = dt.strftime(datetime_pattern)\n",
    "            \n",
    "        return dtStr\n",
    "\n",
    "\n",
    "\n",
    "class InputPreprocessor(object):\n",
    "    \"\"\" The class contains a collection of functions in relation to preprocessing of the historical data \"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloading process of the datasets is implemented by using a __DownloadAdapter__ instance.\n",
    "<br />\n",
    "The __YahooFinanceDownloadAdapter__ and __GoogleFinanceDownloadAdapter__ classes are implementing download datasource specific solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "import io\n",
    "import doctest\n",
    "\n",
    "doctest.testmod()\n",
    "\n",
    "\n",
    "class DownloadAdapter(ABC):\n",
    "    \"\"\" Base class for download adapters\n",
    "    \n",
    "    The subclasses are inherit the 'downloaderFunction' abstract method.\n",
    "    The abstract method's implementation will specify the real download method.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, datasourceName):\n",
    "        super().__init__()\n",
    "        self.datasourceName = datasourceName\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def downloaderFunction(self, symbolCode, startDateTime, endDateTime):\n",
    "        \"\"\" This method will be called by the data manager \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "class YahooFinanceDownloadAdapter(DownloadAdapter):\n",
    "    \"\"\" Specifies Yahoo Finance downloading functionalities by implementing DownloadAdapter \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__('yahoo')\n",
    "    \n",
    "    \n",
    "    def downloaderFunction(self, symbolCode, startDateTime, endDateTime):\n",
    "        \"\"\" This method downloads the historical data from Yahoo Finance\n",
    "        \"\"\"\n",
    "        \n",
    "        # checking parameter type\n",
    "        Utils.checkDatetimeParam(startDateTime, 'startDateTime')\n",
    "        Utils.checkDatetimeParam(endDateTime, 'endDateTime')\n",
    "        \n",
    "        # downloads the historical data\n",
    "        data = pdr.get_data_yahoo(symbolCode, start=startDateTime, end=endDateTime)\n",
    "        \n",
    "        # drops an unnecessary column\n",
    "        del data['Adj Close']\n",
    "        \n",
    "        # sorting table\n",
    "        data.sort_index(inplace=True)\n",
    "        \n",
    "        return data\n",
    "        \n",
    "\n",
    "\n",
    "class GoogleFinanceDownloadAdapter(DownloadAdapter):\n",
    "    \"\"\" Specifies Google Finance downloading functionalities by implementing DownloadAdapter \n",
    "    \n",
    "    This method implements an URL based solution. The reason of not using 'pandas_datareader' library here is that\n",
    "    it seems not to work on older data. According to the experiments a templated direct URL can dowload much more\n",
    "    historical data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__('google')\n",
    "    \n",
    "    \n",
    "    DOWNLOAD_URL_PATTERN = \"http://finance.google.ca/finance/historical?q=%s:%s&startdate=%s&enddate=%s&output=csv\"\n",
    "    DATE_QUERY_PATTERN   = \"%m+%d+%Y\"\n",
    "    \n",
    "    \n",
    "    def downloaderFunction(self, symbolCode, startDateTime, endDateTime):\n",
    "        \"\"\" This method downloads the historical data from Google Finance\n",
    "        \n",
    "        >>> gfda = GoogleFinanceDownloadAdapter()\n",
    "        >>> resultSet = gfda.downloaderFunction(\"GOOGL\", datetime(2016,1,1), datetime(2017,1,1))\n",
    "        >>> len(resultSet)\n",
    "        252\n",
    "        >>> resultSet = gfda.downloaderFunction(\"GOOGL\", datetime(1990,1,1), datetime(2017,1,1))\n",
    "        >>> len(resultSet)\n",
    "        3114\n",
    "        \"\"\"\n",
    "        \n",
    "        # checking parameter type\n",
    "        Utils.checkDatetimeParam(startDateTime, 'startDateTime')\n",
    "        Utils.checkDatetimeParam(endDateTime, 'endDateTime')\n",
    "        \n",
    "        # getting the data \n",
    "        startdate = startDateTime.strftime(self.DATE_QUERY_PATTERN)\n",
    "        enddate = endDateTime.strftime(self.DATE_QUERY_PATTERN)\n",
    "        \n",
    "        stock_url = \"http://finance.google.ca/finance/historical?q=\" + \\\n",
    "                    symbolCode + \"&startdate=\" + startdate + \"&enddate=\" + enddate + \"&output=csv\"\n",
    "        raw_response = requests.get(stock_url).content\n",
    "        \n",
    "        # reading data into pandas dataframe\n",
    "        data = pd.read_csv(io.StringIO(raw_response.decode('utf-8')))\n",
    "                \n",
    "        # identifying the name of the first column\n",
    "        keyColumnName = data.columns[0]    # According to the downloaded CSV this sould be a column named 'Date'\n",
    "\n",
    "        # formatting Date column\n",
    "        def changeDateStr(string):\n",
    "            dt = datetime.strptime(string, '%d-%b-%y')\n",
    "            return dt\n",
    "        data[keyColumnName] = data[keyColumnName].apply(changeDateStr)\n",
    "\n",
    "        # use Date column as key\n",
    "        data.set_index(keyColumnName, inplace=True)\n",
    "\n",
    "        # sorting table\n",
    "        data.sort_index(inplace=True)\n",
    "\n",
    "        return data\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global function __getSP400ListFromWikipedia()__ searches in the web for current list of SP400 companies.\n",
    "\n",
    "The reason of using a webscrapper is that the symbols of the SP400 are changing in time and the script always needs the current values.\n",
    "<br />\n",
    "The current values may be found here: <https://en.wikipedia.org/wiki/List_of_S%26P_400_companies>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a fresh list of 400 companies (changes by time, needs to be scraped at runtime)\n",
    "URL_WIKIPEDIA_SP400 = \"https://en.wikipedia.org/wiki/List_of_S%26P_400_companies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import doctest\n",
    "\n",
    "doctest.testmod()\n",
    "\n",
    "\n",
    "def getSP400ListFromWikipedia():\n",
    "    \"\"\" Returns current SP400 symbol codes from Wikipedia\n",
    "    \n",
    "    Downloads HTML content, selects the first table and returns the values from the first column of the selected table.\n",
    "    \n",
    "    >>> len(getSP400ListFromWikipedia())\n",
    "    400\n",
    "    \"\"\"\n",
    "    \n",
    "    TABLE_NUMBER = 0   # the first table's content should be downloaded from wikipedia\n",
    "    \n",
    "    with urllib.request.urlopen(URL_WIKIPEDIA_SP400) as response:\n",
    "        \n",
    "        # downloading html content\n",
    "        html = response.read()\n",
    "        \n",
    "        # parsing html content\n",
    "        htmlSoup = BeautifulSoup(html, 'lxml')\n",
    "        if htmlSoup is None:\n",
    "            return None\n",
    "        \n",
    "        # finding table in html code\n",
    "        tableHtml = htmlSoup.findAll('table', class_='wikitable sortable')[TABLE_NUMBER]\n",
    "        tableSoup = BeautifulSoup(str(tableHtml), 'lxml')\n",
    "        \n",
    "        # filtering relevant values from table\n",
    "        return [x.td.a.text for x in tableSoup('tr') if x.td]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An instance of the __DataManager__ class can download historical data by using the __DownloadAdapter__'s method.\n",
    "<br />\n",
    "The downloaded historical data are loaded into pandas dataframes, the contents of the dataframes are saved into CSV files in ___data___ directory.\n",
    "<br />\n",
    "Because the download process sometimes fails and not all of the ticker symbols can be found in both of the datasources, therefore a retry logic is implemented which tries to download the files several times, but after that it gives up the process.\n",
    "<br />\n",
    "According to the task's criteria each OHLC record should be downloaded exactly once, therefore a metadata about the properties of the dowload is being kept in an __sqlite__ database. The data is downloaded only if there was no download before for the given symbol name, datasource and the given interval. The downloaded CSV files are kept separately in the _data_ folder (where the name of the files are the keys which contain the symbol name, the interval and the datasource) until a new download process identifies their segmentation and merges them into a common file (if segments are overlaping by time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import itertools\n",
    "import doctest\n",
    "import random\n",
    "import sqlite3\n",
    "import os\n",
    "doctest.testmod()\n",
    "\n",
    "\n",
    "class DataManager:\n",
    "    \"\"\" Downloads and manages historical OHLC and volume data\n",
    "    \n",
    "    Downloads each record exactly only once\n",
    "    Stores the downloaded history as CSV files locally\n",
    "    \n",
    "    TODO: Stores the downloaded history's index in SQL database locally\n",
    "    TODO: Makes queries in the database after the index, checks the already stored data, if data is not present downloads it\n",
    "    \"\"\"\n",
    "    \n",
    "    FILE_NAME_PATTERN = \"%s.%s_%s.%s.csv\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = PlatformLogger('DataManager')\n",
    "        self.dbPath = os.path.join(DATABASE_DIRECTORY_NAME, DATABASE_FILE_NAME)\n",
    "        self.__initializeDataBase()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def __initializeDataBase(self):\n",
    "        \n",
    "        conn = sqlite3.connect(self.dbPath)\n",
    "        conn.execute('''create table if not exists METADATA\n",
    "                (ID              INTEGER PRIMARY KEY,\n",
    "                 SYMBOL_CODE     CHAR(10)   NOT NULL,\n",
    "                 DATASOURCE      TEXT       NOT NULL,\n",
    "                 FILE_NAME       TEXT       NOT NULL,\n",
    "                 INTERVAL_START  TEXT       NOT NULL,\n",
    "                 INTERVAL_END    TEXT       NOT NULL);''')\n",
    "        conn.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def saveDataframeToCsv(self, symbol, dataframe, startDateTime, endDateTime, datasource):\n",
    "        \n",
    "        fileName = DataManager.FILE_NAME_PATTERN % (symbol, \\\n",
    "                                        Utils.convertDateTimeToString(startDateTime), \\\n",
    "                                        Utils.convertDateTimeToString(endDateTime), \\\n",
    "                                        datasource)\n",
    "        filePath = os.path.join(DOWNLOAD_DIRECTORY_NAME, fileName)\n",
    "        dataframe.to_csv(filePath, sep=',', encoding='utf-8')\n",
    "        return filePath\n",
    "    \n",
    "    \n",
    "    \n",
    "    def saveMetadataToDatabase(self, symbol, datasource, filename, intervalStart, intervalEnd):\n",
    "        \n",
    "        conn = sqlite3.connect(self.dbPath)\n",
    "        conn.execute(\"INSERT INTO METADATA (SYMBOL_CODE, DATASOURCE, FILE_NAME, INTERVAL_START, INTERVAL_END) \\\n",
    "              VALUES ('\" + symbol + \"', '\" + datasource + \"', '\" + filename + \"', '\" \n",
    "                          + intervalStart + \"', '\" + intervalEnd + \"' )\");\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "        \n",
    "    def queryDatabase(self, queryString):\n",
    "        \n",
    "        conn = sqlite3.connect(self.dbPath)\n",
    "        cursor = conn.execute(queryString)\n",
    "        result = cursor.fetchall()\n",
    "        conn.close()\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    \n",
    "    def calculateIntervalsToDownload(plannedIntervalToDownload, alreadyStoredIntervals):\n",
    "        \"\"\" Generates time intervals which were not dowloaded before\n",
    "        \n",
    "        Gets an interval (fromDatetime, toDatetime), check the already donwloaded intervals and computes the intervals\n",
    "        which were never downloaded before.\n",
    "        \"\"\"\n",
    "        \n",
    "        deltatime = timedelta(days=1)\n",
    "        ti = IntervalTree([Interval(plannedIntervalToDownload[0], plannedIntervalToDownload[1])])\n",
    "        ti.merge_overlaps()\n",
    "        for storedInterval in alreadyStoredIntervals:\n",
    "            print(storedInterval)\n",
    "            ti.chop(storedInterval[0] - deltatime, storedInterval[1] + deltatime)\n",
    "        return sorted(ti)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def downloadSymbolData(self, listOfSymbols, adapterList, intervalStart, intervalEnd, maxRetryCnt = 10):\n",
    "        \"\"\" Tries to download data for ticker symbols\n",
    "        \n",
    "        The function generates download tasks for each (symbol, adapter, max_retry) triplets and according to \n",
    "        the generated task list it tries to download the historical data. The donwloaded data is saved into\n",
    "        the 'data' directory, the metadata (e.q. interval borders, filename etc.) is saved into database.\n",
    "        \"\"\"\n",
    "             \n",
    "        adapterNames = list(map(lambda x: x.datasourceName, adapterList))\n",
    "        \n",
    "        # if the adapters are not uniquely named raise error\n",
    "        if (len(adapterList) != len(set(adapterNames))):\n",
    "            raise ValueError('The adapters are not unique.')\n",
    "        \n",
    "        # generate the tasks for downloading\n",
    "        dataToDownload = set([x for x in itertools.product(listOfSymbols, adapterNames, [maxRetryCnt])])\n",
    "        \n",
    "        # executing the download tasks, where a download task looks like:\n",
    "        # (symbolCode, datasource, cntOfRemainingRetries)    e.g. ('AAPL', 'google', '5')\n",
    "        while ( 0 < len(dataToDownload)):\n",
    "\n",
    "            # seleting a random task\n",
    "            selectedTask = random.sample(set(dataToDownload), 1)[0]\n",
    "            symbol, datasource, retryCnt = selectedTask\n",
    "            dataToDownload.discard(selectedTask)\n",
    "            \n",
    "            try:\n",
    "                for adapter in adapterList:\n",
    "                    \n",
    "                    # execute the download process with the proper adapter and save file and metadata\n",
    "                    if (adapter.datasourceName == datasource):\n",
    "                        data = adapter.downloaderFunction(symbol, intervalStart, intervalEnd)                    \n",
    "                        filePath = self.saveDataframeToCsv(symbol, data, intervalStart, intervalEnd, datasource)\n",
    "                        self.saveMetadataToDatabase(symbol, datasource, filePath, \\\n",
    "                                        Utils.convertDateTimeToString(intervalStart), \\\n",
    "                                        Utils.convertDateTimeToString(intervalEnd))\n",
    "                        self.logger.info(\"downloaded: \" + str((symbol, datasource)) + \"\\t\" + filePath)\n",
    "                        break\n",
    "            \n",
    "            except:\n",
    "                \n",
    "                # if the selected download task was not executed, the retry count is decremented\n",
    "                if (retryCnt > 1):\n",
    "                    dataToDownload.add((symbol, datasource, retryCnt-1))\n",
    "                counter = maxRetryCnt - retryCnt + 1\n",
    "                self.logger.warning(\"failed to download [\" + str(counter) + \"/\" + str(maxRetryCnt) + \"]: \" \\\n",
    "                               + str((symbol, datasource)))\n",
    "                \n",
    "    \n",
    "    \n",
    "    def downloadData(self, downloaderFunction, datasource=None):\n",
    "        \"\"\" Returns a method with curried parameters which can download historical data \"\"\"\n",
    "        \n",
    "        def downloader(symbolCode, startDateTime, endDateTime):\n",
    "            \"\"\" Returns a closure with fixed downloaderFunction and datasource \"\"\"\n",
    "            return downloaderFunction(symbolCode, startDateTime, endDateTime, datasource)\n",
    "        \n",
    "        return downloader\n",
    "    \n",
    "    \n",
    "    \n",
    "    def getData(self, symbolOrSymbolList, startDateTime, endDateTime):\n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interval border constants\n",
    "DATA_START          = datetime(2016,1,1)\n",
    "DATA_END            = datetime(2017,1,1)\n",
    "DATA_VARIANCE_START = datetime(2016,2,11)\n",
    "DATA_VARIANCE_END   = datetime(2016,11,8)\n",
    "DATA_MINMAX_START   = datetime(2016,1,18)    # including\n",
    "DATA_MINMAX_END     = datetime(2016,10,18)   # excluding\n",
    "DATA_STD_START      = datetime(2016,4,17)\n",
    "DATA_STD_END        = datetime(2016,12,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution test\n",
    "listOfSymbols = getSP400ListFromWikipedia()\n",
    "truncList1 = listOfSymbols[:3]\n",
    "truncList2 = listOfSymbols[10:13]\n",
    "\n",
    "gfda = GoogleFinanceDownloadAdapter()\n",
    "yfda = YahooFinanceDownloadAdapter()\n",
    "\n",
    "dm1 = DataManager()\n",
    "dm1.downloadSymbolData(truncList1, [gfda, yfda], DATA_START, DATA_END, 5)\n",
    "dm2 = DataManager()\n",
    "dm2.downloadSymbolData(truncList2, [gfda, yfda], DATA_START, DATA_END, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.queryDatabase(\"select * from METADATA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
