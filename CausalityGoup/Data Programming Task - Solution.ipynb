{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution - László Pogány"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason of selecting __Jupyter/IPython Notebook__ against a simple Python project is the self documentation support. \n",
    "Although the solutions implemented in this way can't be integrated directly into the existing operative systems (without further work), but it is much easier to demonstrate the whole process and the implemented functionalities step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution installs the external libraries, downloads the required historical data automatically and computes the relevant metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version\n",
    "# Python 3.6.3 :: Anaconda, Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __pandas__ library is used for storing the data in dataframes.\n",
    "<br />\n",
    "The __beautifulsoup4__ library is used for scrapping data from web.\n",
    "<br />\n",
    "The __pandas-datareader__ library is used for downloading __Yahoo Finance__ and __Google Finance__ datasets.<br />\n",
    "Further information: https://github.com/pydata/pandas-datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datetime\n",
    "!pip install bs4\n",
    "!pip install requests\n",
    "!pip install numpy\n",
    "!pip install scipy\n",
    "!pip install pandas\n",
    "!pip install pandas-datareader\n",
    "!pip install intervaltree\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My local configuration (shown due to the experienced difficulties which are probably related to different versions of packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show datetime | grep -i \"Version:\"\n",
    "!pip show bs4 | grep -i \"Version:\"\n",
    "!pip show requests | grep -i \"Version:\"\n",
    "!pip show numpy | grep -i \"Version:\"\n",
    "!pip show scipy | grep -i \"Version:\"\n",
    "!pip show pandas | grep -i \"Version:\"\n",
    "!pip show pandas-datareader | grep -i \"Version:\"\n",
    "!pip show intervaltree | grep -i \"Version:\"\n",
    "!pip show beautifulsoup4 | grep -i \"Version:\"\n",
    "#Version: 4.2\n",
    "#Version: 0.0.1\n",
    "#Version: 2.18.4\n",
    "#Version: 1.13.3\n",
    "#Version: 0.19.1\n",
    "#Version: 0.20.3\n",
    "#Version: 0.5.0\n",
    "#Version: 2.1.0\n",
    "#Version: 4.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloader and preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predefined directory names for the application.\n",
    "The downloads and the calculated outputs are placed into special directories inside the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# constants and directory structure for the application\n",
    "OUTPUT_DIRECTORY_PATH       = r'..\\out'\n",
    "LOG_DIRECTORY_PATH          = os.path.join(OUTPUT_DIRECTORY_PATH, \"logs\")\n",
    "DOWNLOAD_DIRECTORY_NAME     = os.path.join(OUTPUT_DIRECTORY_PATH, \"data\")\n",
    "DATABASE_DIRECTORY_NAME     = os.path.join(OUTPUT_DIRECTORY_PATH, \"meta\")\n",
    "FINGERPRINTS_DIRECTORY_NAME = os.path.join(OUTPUT_DIRECTORY_PATH, \"calc\")\n",
    "\n",
    "DATABASE_FILE_NAME          = 'metadata.db'\n",
    "\n",
    "def createDirectory(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "createDirectory(OUTPUT_DIRECTORY_PATH)\n",
    "createDirectory(LOG_DIRECTORY_PATH)\n",
    "createDirectory(DOWNLOAD_DIRECTORY_NAME)\n",
    "createDirectory(DATABASE_DIRECTORY_NAME)\n",
    "createDirectory(FINGERPRINTS_DIRECTORY_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __PlatformLogger__ function provides logging services for the modules/classes. <br />\n",
    "The __Utils__ and __InputPreprocessor__ classes are implementing common functionalities for data manipulation in form of static helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import doctest\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "doctest.testmod()\n",
    "\n",
    "\n",
    "loggers = {}\n",
    "def PlatformLogger(moduleName):\n",
    "    \"\"\" Provides logging functionalities.\n",
    "    \n",
    "    The class configures the log service, the log handlers and returns a logger object.\n",
    "    \"\"\"\n",
    "    \n",
    "    global loggers\n",
    "\n",
    "    if loggers.get(moduleName):\n",
    "        \n",
    "        return loggers.get(moduleName)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # create logger with moduleName parameter\n",
    "        logger = logging.getLogger(moduleName)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "\n",
    "        # create console handler with a higher log level\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(logging.INFO)\n",
    "\n",
    "        # create file handler which logs even debug messages\n",
    "        fh = logging.FileHandler(os.path.join(LOG_DIRECTORY_PATH, 'error.log'))\n",
    "        fr = logging.handlers.RotatingFileHandler(os.path.join(LOG_DIRECTORY_PATH, 'event.log'))\n",
    "        fh.setLevel(logging.WARN)\n",
    "        fr.setLevel(logging.DEBUG)\n",
    "\n",
    "        # create formatter and add it to the handlers\n",
    "        formatter = logging.Formatter('[%(asctime)s] - %(name)s - %(levelname)s - %(message)s')\n",
    "        fh.setFormatter(formatter)\n",
    "        fr.setFormatter(formatter)\n",
    "        ch.setFormatter(formatter)\n",
    "\n",
    "        # add the handlers to the logger\n",
    "        logger.addHandler(fh)\n",
    "        logger.addHandler(fr)\n",
    "        logger.addHandler(ch)\n",
    "        \n",
    "        # updating loggers\n",
    "        loggers.update({moduleName: logger})\n",
    "\n",
    "        return logger\n",
    "\n",
    "\n",
    "\n",
    "class Utils(object):\n",
    "    \"\"\" The class contains a collection of useful functions in relation to minor data manipulations \"\"\"\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def checkDatetimeParam(dt, paramName):\n",
    "        assert (type(dt) is datetime), \\\n",
    "            paramName + \" parameter is not datetime, it is a(n) %s\" % type(dt)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def checkStringParam(string, paramName):\n",
    "        assert (type(string) is str), \\\n",
    "            paramName + \" parameter is not str, it is a(n) %s\" % type(dt)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def convertDateTimeToUnixTimeStamp(dt, useLocalTime=False):\n",
    "        if (useLocalTime):\n",
    "            return int(time.mktime(dt.timetuple()))\n",
    "        else:\n",
    "            return dt.replace(tzinfo=timezone.utc).timestamp()\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def convertUnixTimeStampToDateTime(ts, useLocalTime=False):\n",
    "        if (useLocalTime):\n",
    "            return datetime.fromtimestamp(ts)\n",
    "        else:\n",
    "            return datetime.utcfromtimestamp(ts)\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def convertDatetimeStrFormat(dtString, fromPattern, toPattern):\n",
    "        \"\"\" The method converts a datetime object represented in formatted string to another formatted string \n",
    "        \n",
    "        >>> Utils.convertDatetimeStrFormat('1987-08-14', \"%Y-%m-%d\", \"%d/%m/%Y\")\n",
    "        '14/08/1987'\n",
    "        >>> Utils.convertDatetimeStrFormat('5/1/1990', \"%d/%m/%Y\", \"%Y-%m-%d\")\n",
    "        '1990-01-05'\n",
    "        \"\"\"\n",
    "        \n",
    "        return datetime.strptime(dtString, fromPattern).strftime(toPattern)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def convertDateTimeToString(dt, datetime_pattern = \"%Y%m%d\"):\n",
    "        \"\"\" Converter for datetime types\n",
    "        \n",
    "        In case of datetime parameter returns a formatted datetime string, otherwise returns the original\n",
    "        \n",
    "        >>> Utils.convertDateTimeToString(datetime(2016,1,2,3,4,5))\n",
    "        '20160102'\n",
    "        >>> Utils.convertDateTimeToString(datetime(2016,1,2,3,4,5), \"%Y%m%d-%H%M%S\")\n",
    "        '20160102-030405'\n",
    "        >>> Utils.convertDateTimeToString('2016-01-01')\n",
    "        '2016-01-01'\n",
    "        >>> Utils.convertDateTimeToString(2016)\n",
    "        Traceback (most recent call last):\n",
    "          ...\n",
    "        AssertionError: dt parameter is not string nor datetime, it is a(n) <class 'int'>\n",
    "        \"\"\"\n",
    "        \n",
    "        # checking parameter type\n",
    "        assert (type(dt) is datetime or type(dt) is str), \\\n",
    "            \"dt parameter is not string nor datetime, it is a(n) %s\" % type(dt)\n",
    "        \n",
    "        # converversion with formatting datetime to string if needed\n",
    "        dtStr = dt\n",
    "        if(type(dt) is datetime):\n",
    "            dtStr = dt.strftime(datetime_pattern)\n",
    "            \n",
    "        return dtStr\n",
    "\n",
    "\n",
    "\n",
    "class InputPreprocessor(object):\n",
    "    \"\"\" The class contains a collection of functions in relation to preprocessing of the historical data \"\"\"\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def cleanDataframe(dataframe, uniqueColNameList = None):\n",
    "        \"\"\" Sorts, reindexes the dataframe and removes duplicates \"\"\"\n",
    "        \n",
    "        if (uniqueColNameList == None):\n",
    "            uniqueColNameList = dataframe.columns[0]\n",
    "        \n",
    "        # dropping duplicates, sorting and reindexing the table\n",
    "        dataframe.drop_duplicates(subset=uniqueColNameList, inplace=True)\n",
    "        dataframe.sort_values(uniqueColNameList, inplace=True)\n",
    "        dataframe.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def filterOutliers(candles, SIGMA = 3):\n",
    "        \"\"\" Filters the outliers from the dataset according to N-sigma rule\n",
    "        \n",
    "        Determines the outliers by using the standard deviation and checks the non-zero condition on prices.\n",
    "        Args:\n",
    "            candles: the original candle dataset with columns: 'Open', 'Close', 'High', 'Low', 'Volume'.\n",
    "        Returns:\n",
    "            valid: dataframe of correct candles with a new index.\n",
    "            filtered: dataframe of filtered candles with the original index.\n",
    "            valid: dataframe of correct candles with the original index.\n",
    "        \"\"\"\n",
    "        \n",
    "        # abbreviating columns\n",
    "        o = candles[\"Open\"]\n",
    "        h = candles[\"High\"]\n",
    "        l = candles[\"Low\"]\n",
    "        c = candles[\"Close\"]\n",
    "        v = volumes = candles[\"Volume\"]\n",
    "        \n",
    "        # concatenating price columns\n",
    "        aggr_prices = pd.concat([o, h, l, c])\n",
    "           \n",
    "        # calculating scalars for outlier detection\n",
    "        price_mean = aggr_prices.mean()\n",
    "        price_std  = aggr_prices.std()\n",
    "        vols_mean  = volumes.mean()\n",
    "        vols_std   = volumes.std()\n",
    "        \n",
    "        # getting outliers for prices\n",
    "        x_o1 = candles[~(np.abs(o - price_mean) <= (SIGMA * price_std))]\n",
    "        x_c1 = candles[~(np.abs(c - price_mean) <= (SIGMA * price_std))]\n",
    "        x_h1 = candles[~(np.abs(h - price_mean) <= (SIGMA * price_std))]\n",
    "        x_l1 = candles[~(np.abs(l - price_mean) <= (SIGMA * price_std))]\n",
    "        x_o2 = candles[o <= 0.0]\n",
    "        x_c2 = candles[h <= 0.0]\n",
    "        x_h2 = candles[l <= 0.0]\n",
    "        x_l2 = candles[c <= 0.0]\n",
    "        \n",
    "        # getting outliers for volumes\n",
    "        x_v1 = candles[~(np.abs(v - vols_mean) <= (SIGMA * vols_std))]\n",
    "        x_v2 = candles[v <= 0.0]\n",
    "        \n",
    "        # getting filtered outliers\n",
    "        filtered = x_o1.append(x_o2)\n",
    "        filtered = filtered.append(x_c1).append(x_c2)\n",
    "        filtered = filtered.append(x_h1).append(x_h2)\n",
    "        filtered = filtered.append(x_l1).append(x_l2)\n",
    "        filtered = filtered.append(x_v1).append(x_v2)\n",
    "        \n",
    "        # getting valid data\n",
    "        filtered = filtered.drop_duplicates()\n",
    "        valid = candles[~candles.isin(filtered)].dropna()\n",
    "        \n",
    "        # returns: valid data with new index, filtered data with old index, valid data with old index\n",
    "        return valid.reset_index(drop=True), filtered, valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloading process of the datasets is implemented by using a __DownloadAdapter__ instance.\n",
    "<br />\n",
    "The __YahooFinanceDownloadAdapter__ and __GoogleFinanceDownloadAdapter__ classes are implementing download datasource specific solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "import io\n",
    "import doctest\n",
    "\n",
    "doctest.testmod()\n",
    "\n",
    "\n",
    "class DownloadAdapter(ABC):\n",
    "    \"\"\" Base class for download adapters\n",
    "    \n",
    "    The subclasses are inherit the 'downloaderFunction' abstract method.\n",
    "    The abstract method's implementation will specify the real download method.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, datasourceName):\n",
    "        super().__init__()\n",
    "        self.datasourceName = datasourceName\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def downloaderFunction(self, symbolCode, startDateTime, endDateTime):\n",
    "        \"\"\" This method will be called by the data manager \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "class YahooFinanceDownloadAdapter(DownloadAdapter):\n",
    "    \"\"\" Specifies Yahoo Finance downloading functionalities by implementing DownloadAdapter \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__('yahoo')\n",
    "    \n",
    "    \n",
    "    def downloaderFunction(self, symbolCode, startDateTime, endDateTime):\n",
    "        \"\"\" This method downloads the historical data from Yahoo Finance\n",
    "        \"\"\"\n",
    "        \n",
    "        # checking parameter type\n",
    "        Utils.checkDatetimeParam(startDateTime, 'startDateTime')\n",
    "        Utils.checkDatetimeParam(endDateTime, 'endDateTime')\n",
    "        \n",
    "        # downloads the historical data\n",
    "        data = pdr.get_data_yahoo(symbolCode, start=startDateTime, end=endDateTime)\n",
    "        \n",
    "        # drops an unnecessary column\n",
    "        del data['Adj Close']\n",
    "        \n",
    "        # sorting table\n",
    "        data.sort_index(inplace=True)\n",
    "        \n",
    "        return data\n",
    "        \n",
    "\n",
    "\n",
    "class GoogleFinanceDownloadAdapter(DownloadAdapter):\n",
    "    \"\"\" Specifies Google Finance downloading functionalities by implementing DownloadAdapter \n",
    "    \n",
    "    This method implements an URL based solution. The reason of not using 'pandas_datareader' library here is that\n",
    "    it seems not to work on older data. According to the experiments a templated direct URL can dowload much more\n",
    "    historical data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__('google')\n",
    "    \n",
    "    \n",
    "    DOWNLOAD_URL_PATTERN = \"http://finance.google.ca/finance/historical?q=%s:%s&startdate=%s&enddate=%s&output=csv\"\n",
    "    DATE_QUERY_PATTERN   = \"%m+%d+%Y\"\n",
    "    \n",
    "    \n",
    "    def downloaderFunction(self, symbolCode, startDateTime, endDateTime):\n",
    "        \"\"\" This method downloads the historical data from Google Finance\n",
    "        \n",
    "        >>> gfda = GoogleFinanceDownloadAdapter()\n",
    "        >>> resultSet = gfda.downloaderFunction(\"GOOGL\", datetime(2016,1,1), datetime(2017,1,1))\n",
    "        >>> len(resultSet)\n",
    "        252\n",
    "        >>> resultSet = gfda.downloaderFunction(\"GOOGL\", datetime(1990,1,1), datetime(2017,1,1))\n",
    "        >>> len(resultSet)\n",
    "        3114\n",
    "        \"\"\"\n",
    "        \n",
    "        # checking parameter type\n",
    "        Utils.checkDatetimeParam(startDateTime, 'startDateTime')\n",
    "        Utils.checkDatetimeParam(endDateTime, 'endDateTime')\n",
    "        \n",
    "        # getting the data \n",
    "        startdate = startDateTime.strftime(self.DATE_QUERY_PATTERN)\n",
    "        enddate = endDateTime.strftime(self.DATE_QUERY_PATTERN)\n",
    "        \n",
    "        stock_url = \"http://finance.google.ca/finance/historical?q=\" + \\\n",
    "                    symbolCode + \"&startdate=\" + startdate + \"&enddate=\" + enddate + \"&output=csv\"\n",
    "        raw_response = requests.get(stock_url).content\n",
    "        \n",
    "        # reading data into pandas dataframe\n",
    "        data = pd.read_csv(io.StringIO(raw_response.decode('utf-8')))\n",
    "                \n",
    "        # identifying the name of the first column\n",
    "        keyColumnName = data.columns[0]    # According to the downloaded CSV this sould be a column named 'Date'\n",
    "\n",
    "        # formatting Date column\n",
    "        def changeDateStr(string):\n",
    "            dt = datetime.strptime(string, '%d-%b-%y')\n",
    "            return dt\n",
    "        data[keyColumnName] = data[keyColumnName].apply(changeDateStr)\n",
    "\n",
    "        # use Date column as key\n",
    "        data.set_index(keyColumnName, inplace=True)\n",
    "\n",
    "        # sorting table\n",
    "        data.sort_index(inplace=True)\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global function __getSP400ListFromWikipedia()__ searches in the web for current list of SP400 companies.\n",
    "\n",
    "The reason of using a webscrapper is that the symbols of the SP400 are changing in time and the script always needs the current values.\n",
    "<br />\n",
    "The current values may be found here: <https://en.wikipedia.org/wiki/List_of_S%26P_400_companies>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a fresh list of 400 companies (changes by time, needs to be scraped at runtime)\n",
    "URL_WIKIPEDIA_SP400 = \"https://en.wikipedia.org/wiki/List_of_S%26P_400_companies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import doctest\n",
    "\n",
    "doctest.testmod()\n",
    "\n",
    "\n",
    "def getSP400ListFromWikipedia():\n",
    "    \"\"\" Returns current SP400 symbol codes from Wikipedia\n",
    "    \n",
    "    Downloads HTML content, selects the first table and returns the values from the first column of the selected table.\n",
    "    \n",
    "    >>> len(getSP400ListFromWikipedia())\n",
    "    400\n",
    "    \"\"\"\n",
    "    \n",
    "    TABLE_NUMBER = 0   # the first table's content should be downloaded from wikipedia\n",
    "    \n",
    "    with urllib.request.urlopen(URL_WIKIPEDIA_SP400) as response:\n",
    "        \n",
    "        # downloading html content\n",
    "        html = response.read()\n",
    "        \n",
    "        # parsing html content\n",
    "        htmlSoup = BeautifulSoup(html, 'lxml')\n",
    "        if htmlSoup is None:\n",
    "            return None\n",
    "        \n",
    "        # finding table in html code\n",
    "        tableHtml = htmlSoup.findAll('table', class_='wikitable sortable')[TABLE_NUMBER]\n",
    "        tableSoup = BeautifulSoup(str(tableHtml), 'lxml')\n",
    "        \n",
    "        # filtering relevant values from table\n",
    "        return [x.td.a.text for x in tableSoup('tr') if x.td]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An instance of the __DataManager__ class can download historical data by using the __DownloadAdapter__'s method.\n",
    "<br />\n",
    "The downloaded historical data are loaded into pandas dataframes, the contents of the dataframes are saved into CSV files in ___data___ directory.\n",
    "<br />\n",
    "Because the download process sometimes fails and not all of the ticker symbols can be found in both of the datasources, therefore a retry logic is implemented which tries to download the files several times, but after that it gives up the process.\n",
    "<br />\n",
    "According to the task's criteria each OHLCV record should be downloaded exactly once, therefore a metadata about the properties of the dowload is being kept in an __sqlite__ database. The data is downloaded only if there was no download before for the given symbol name, datasource and the given interval. The downloaded CSV files are kept separately in the _data_ folder (where the name of the files are the keys which contain the symbol name, the interval and the datasource) until a new download process identifies their segmentation and merges them into a common file (if segments are overlaping by time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import itertools\n",
    "from intervaltree import Interval, IntervalTree\n",
    "import doctest\n",
    "import random\n",
    "import sqlite3\n",
    "import os\n",
    "doctest.testmod()\n",
    "\n",
    "\n",
    "class DataManager:\n",
    "    \"\"\" Downloads and manages historical OHLCV and volume data\n",
    "    \n",
    "    Downloads each record exactly only once.\n",
    "    Stores the downloaded history as CSV files locally.\n",
    "    Stores the downloaded history's index in SQL database locally.\n",
    "    Makes queries in the database after the index, checks the already stored data, if data is not present downloads it.\n",
    "    The sqlite does not have datetime column type, text or integer type can be used for storing datetimes.\n",
    "    \n",
    "    TODOs:\n",
    "    - Since INTEGER based UNIX timestamps are used in metadata database, TEXT based datetimes can be removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    FILE_NAME_PATTERN = \"%s.%s_%s.%s.csv\"\n",
    "    \n",
    "    def __init__(self, downloadAdapterList = None):\n",
    "        self.logger = PlatformLogger('DataManager')\n",
    "        self.__checkAdapterUniqueness(downloadAdapterList)\n",
    "        self.downloadAdapterList = downloadAdapterList\n",
    "        self.dbPath = os.path.join(DATABASE_DIRECTORY_NAME, DATABASE_FILE_NAME)\n",
    "        self.__initializeDataBase()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def __initializeDataBase(self):\n",
    "        \"\"\" Initializes the database for metadata if needed \"\"\"\n",
    "        \n",
    "        conn = sqlite3.connect(self.dbPath)\n",
    "        conn.execute('''create table if not exists METADATA\n",
    "                (ID                 integer    primary key,\n",
    "                 SYMBOL_CODE        char(10)   not null,\n",
    "                 DATASOURCE         text       not null,\n",
    "                 FILE_NAME          text       not null,\n",
    "                 INTERVAL_START     text       not null,\n",
    "                 INTERVAL_END       text       not null,\n",
    "                 INTERVAL_START_TS  integer    not null,\n",
    "                 INTERVAL_END_TS    integer    not null);''')\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "        \n",
    "    def __queryDatabase(self, queryString):\n",
    "        \"\"\" Executes a query statement and returns the result set \"\"\"\n",
    "        \n",
    "        conn = sqlite3.connect(self.dbPath)\n",
    "        cursor = conn.execute(queryString)\n",
    "        result = cursor.fetchall()\n",
    "        conn.close()\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __saveDataframeToCsv(self, symbol, dataframe, startDateTime, endDateTime, datasourceName, \\\n",
    "                             colNames = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']):\n",
    "        \"\"\" Saves a pandas dataframe into a CSV file \"\"\"\n",
    "        \n",
    "        Utils.checkStringParam(symbol, 'symbol')\n",
    "        Utils.checkStringParam(datasourceName, 'datasourceName')\n",
    "        Utils.checkDatetimeParam(startDateTime, 'startDateTime')\n",
    "        Utils.checkDatetimeParam(endDateTime, 'endDateTime')\n",
    "        \n",
    "        # in downloaded dataframe the 'Date' column is used as key (due to the used API)\n",
    "        assert(len(dataframe.columns) == len(colNames)-1), \\\n",
    "            \"dataframe's column count (%s) does not match to given number of columns %s\" % \\\n",
    "            (len(dataframe.columns), len(colNames))\n",
    "            \n",
    "        dataframe.index.name = colNames[0]\n",
    "        dataframe.columns = colNames[1:]\n",
    "        \n",
    "        fileName = DataManager.FILE_NAME_PATTERN % (symbol, \\\n",
    "                                        Utils.convertDateTimeToString(startDateTime), \\\n",
    "                                        Utils.convertDateTimeToString(endDateTime), \\\n",
    "                                        datasourceName)\n",
    "        filePath = os.path.join(DOWNLOAD_DIRECTORY_NAME, fileName)\n",
    "        dataframe.to_csv(filePath, sep=',', encoding='utf-8')\n",
    "        return filePath\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __saveMetadataToDatabase(self, symbol, datasourceName, fileName,\n",
    "                                 intervalStart, intervalEnd, intervalStartTs, intervalEndTs):\n",
    "        \"\"\" Inserts a row into METADATA table in metadata.db\"\"\"\n",
    "        \n",
    "        Utils.checkStringParam(symbol, 'symbol')\n",
    "        Utils.checkStringParam(datasourceName, 'datasourceName')\n",
    "        Utils.checkStringParam(fileName, 'fileName')\n",
    "        Utils.checkStringParam(intervalStart, 'intervalStart')\n",
    "        Utils.checkStringParam(intervalEnd, 'intervalEnd')\n",
    "        Utils.checkStringParam(intervalStartTs, 'intervalStartTs')\n",
    "        Utils.checkStringParam(intervalEndTs, 'intervalEndTs')\n",
    "        \n",
    "        conn = sqlite3.connect(self.dbPath)\n",
    "        conn.execute(\"INSERT INTO METADATA (SYMBOL_CODE, DATASOURCE, FILE_NAME, \" + \\\n",
    "                     \"INTERVAL_START, INTERVAL_END, INTERVAL_START_TS, INTERVAL_END_TS) \\\n",
    "              VALUES ('\" + symbol + \"', '\" + datasourceName + \"', '\" + fileName + \"', '\" \n",
    "                         + intervalStart + \"', '\" + intervalEnd + \"', '\" \n",
    "                         + intervalStartTs + \"', '\" + intervalEndTs + \"' )\");\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __calculateIntervalsToDownload(self, plannedIntervalToDownload, alreadyStoredIntervals):\n",
    "        \"\"\" Generates time intervals which were not dowloaded before\n",
    "        \n",
    "        Gets an interval Tuple(fromDatetime, toDatetime), check the already donwloaded intervals \n",
    "        List[Tuple(fromDatetime, toDatetime)] and computes the intervals which were never downloaded before.\n",
    "        \"\"\"\n",
    "        \n",
    "        # an offset must be used due to the inclusive interval borders of already downloaded data\n",
    "        deltatime = timedelta(days=1)\n",
    "        \n",
    "        # planned interval will be choped by already stored intervals\n",
    "        ti = IntervalTree([Interval(plannedIntervalToDownload[0], plannedIntervalToDownload[1])])\n",
    "        ti.merge_overlaps()\n",
    "        \n",
    "        # choping all previously downloaded interval\n",
    "        for storedInterval in alreadyStoredIntervals:\n",
    "            ti.chop(storedInterval[0] - deltatime, storedInterval[1] + deltatime)\n",
    "            \n",
    "        return sorted(ti)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __checkAdapterUniqueness(self, adapterList):\n",
    "        \"\"\" Checks if the list of adapters are uniquely named \n",
    "        \n",
    "        The unique naming of the adapters ensures the different values for 'datasource' property in database.\n",
    "        If the naming is unique, returns their names.\n",
    "        \"\"\"\n",
    "        \n",
    "        if (adapterList != None):\n",
    "            adapterNames = list(map(lambda x: x.datasourceName, adapterList))\n",
    "\n",
    "            # if the adapters are not uniquely named raise error\n",
    "            if (len(adapterList) != len(set(adapterNames))):\n",
    "                raise ValueError('The adapters are not unique.')\n",
    "            \n",
    "            return adapterNames\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __downloadSymbolData(self, listOfSymbols, adapterList, intervalStart, intervalEnd, maxRetryCnt = 10):\n",
    "        \"\"\" Tries to download data for ticker symbols\n",
    "        \n",
    "        The function generates download tasks for each (symbol, adapter, max_retry) triplets and according to \n",
    "        the generated task list it tries to download the historical data. The donwloaded data is saved into\n",
    "        the 'data' directory, the metadata (e.q. interval borders, filename etc.) is saved into database.\n",
    "        \"\"\"\n",
    "        \n",
    "        if(len(adapterList) == 0):\n",
    "            raise ValueError('There is no adapter to download the data with.')\n",
    "             \n",
    "        adapterNames = self.__checkAdapterUniqueness(adapterList)\n",
    "        \n",
    "        # generate the tasks for downloading\n",
    "        dataToDownload = set([x for x in itertools.product(listOfSymbols, adapterNames, [maxRetryCnt])])\n",
    "        \n",
    "        # executing the download tasks, where a download task looks like:\n",
    "        # (symbolCode, datasource, cntOfRemainingRetries)    e.g. ('AAPL', 'google', '5')\n",
    "        while ( 0 < len(dataToDownload)):\n",
    "\n",
    "            # seleting a random task\n",
    "            selectedTask = random.sample(set(dataToDownload), 1)[0]\n",
    "            symbol, datasource, retryCnt = selectedTask\n",
    "            dataToDownload.discard(selectedTask)\n",
    "            \n",
    "            try:\n",
    "                for adapter in adapterList:\n",
    "                    \n",
    "                    # execute the download process with the proper adapter and save file and metadata\n",
    "                    if (adapter.datasourceName == datasource):\n",
    "                        data = adapter.downloaderFunction(symbol, intervalStart, intervalEnd)                    \n",
    "                        filePath = self.__saveDataframeToCsv(symbol, data, intervalStart, intervalEnd, datasource)\n",
    "                        self.__saveMetadataToDatabase(symbol, datasource, filePath, \\\n",
    "                                        Utils.convertDateTimeToString(intervalStart), \\\n",
    "                                        Utils.convertDateTimeToString(intervalEnd), \\\n",
    "                                        str(Utils.convertDateTimeToUnixTimeStamp(intervalStart)), \\\n",
    "                                        str(Utils.convertDateTimeToUnixTimeStamp(intervalEnd)))\n",
    "                        self.logger.info(\"downloaded: \" + str((symbol, datasource)) + \"\\t\" + filePath)\n",
    "                        break\n",
    "            \n",
    "            except:\n",
    "                \n",
    "                # if the selected download task was not executed, the retry count is decremented\n",
    "                if (retryCnt > 1):\n",
    "                    dataToDownload.add((symbol, datasource, retryCnt-1))\n",
    "                counter = maxRetryCnt - retryCnt + 1\n",
    "                self.logger.warning(\"failed to download [\" + str(counter) + \"/\" + str(maxRetryCnt) + \"]: \" \\\n",
    "                               + str((symbol, datasource)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    def templatedSelectQuery(self, selectQuery):\n",
    "        \"\"\" Returns a templated function with two parameters to fill \"\"\"\n",
    "        \n",
    "        def concreteSelectQuery(selectColumnList, whereClauseParameterTuple):\n",
    "            \"\"\" Executes the query with two parameters \n",
    "            \n",
    "            The first parameter is a list of colums to select which will be joined into string inside a 1-tuple.\n",
    "            The second parameter is a N-tuple containing parameter values for the given selectQuery.\n",
    "            Eventualy 1+N parameters will be substituted into query string (1 from first parameter, N from the second).\n",
    "            \"\"\"\n",
    "            \n",
    "            params = (\", \".join(selectColumnList), ) + whereClauseParameterTuple           \n",
    "            query = selectQuery % params\n",
    "            return self.__queryDatabase(query)\n",
    "        \n",
    "        return concreteSelectQuery\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __readMultipleCSVsIntoSingleDataframe(self, listOfCSVs):\n",
    "        \"\"\" Concatenates multiple dataframes into one big dataframe \"\"\"\n",
    "        \n",
    "        # concatenating dataframes\n",
    "        dataframes = map(lambda x: pd.read_csv(x), listOfCSVs)       \n",
    "        retDf = pd.concat(dataframes)\n",
    "        \n",
    "        # converting column type\n",
    "        dateColName = retDf.columns[0]\n",
    "        retDf[dateColName] = pd.to_datetime(retDf[dateColName], format=\"%Y-%m-%d\")\n",
    "        return retDf\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __dropIntervalInplace(self, df, intervalStart, intervalEnd, \\\n",
    "                       inclusiveFilterStart = False, inclusiveFilterEnd = False):\n",
    "        \"\"\" Drops unnecessary rows from dataframe \"\"\"\n",
    "        \n",
    "        dateColName = df.columns[0]\n",
    "        \n",
    "        # filtering the rows in case of CSV's contents are covering a wider interval\n",
    "        if (inclusiveFilterStart):\n",
    "            maskStart = df[dateColName] <= intervalStart\n",
    "        else:\n",
    "            maskStart = df[dateColName] < intervalStart\n",
    "        if (inclusiveFilterEnd):\n",
    "            maskEnd = df[dateColName] >= intervalEnd\n",
    "        else:\n",
    "            maskEnd = df[dateColName] > intervalEnd\n",
    "        mask = maskStart | maskEnd\n",
    "        df.drop(df[mask].index, inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def getData(self, symbolOrSymbolList, interval, downloadAdapterList = None, maxRetryCnt = 10, \\\n",
    "                startInclusiveFilter = False, endInclusiveFilter = False):\n",
    "        \"\"\" Returns dataframes for the requested time interval\n",
    "        \n",
    "        According to the information stored in metadata determines the intervals which were never dowloaded before.\n",
    "        Downloads the unseen data. Combines the relevant (already stored and downloaded) data into a pandas dataframe.\n",
    "        \n",
    "        The returned dictionaries can be addressed by the following ways:\n",
    "        returnMultiIndex[_]                    # data can be accessed by any addressing of the following solutions\n",
    "        returnTupleDict0[('ANN', 'google')]    # in case of both parameters are known\n",
    "        returnMultiDict1['ANN']['google']      # in case of symbol parameter is known\n",
    "        returnMultiDict2['google']['ANN']      # in case of datasource parameter is known\n",
    "        \"\"\"\n",
    "        \n",
    "        # checking the input parameters\n",
    "        if (isinstance(symbolOrSymbolList, str)):\n",
    "            symbolOrSymbolList = [symbolOrSymbolList]\n",
    "            \n",
    "        assert (isinstance(symbolOrSymbolList, (list, tuple))), \\\n",
    "            \"symbolOrSymbolList parameter is not list nor tuple, it is a(n) %s\" % type(symbolOrSymbolList)\n",
    "        assert (len(interval) == 2), \\\n",
    "            \"interval parameter is not a tuple with size of 2, it's size is %s\" % len(interval)\n",
    "            \n",
    "        Utils.checkDatetimeParam(interval[0], \"interval[0]\")\n",
    "        Utils.checkDatetimeParam(interval[1], \"interval[1]\")\n",
    "                \n",
    "        if (downloadAdapterList == None or len(downloadAdapterList) == 0):\n",
    "            downloadAdapterList = self.downloadAdapterList\n",
    "        \n",
    "        # logging the valid request for getting data\n",
    "        adapterNames = self.__checkAdapterUniqueness(downloadAdapterList)\n",
    "        self.logger.info(\"getting data: \" + str(adapterNames) + \"\\t\" \\\n",
    "                         + str(symbolOrSymbolList) + \"\\t\" + str(interval))\n",
    "        \n",
    "        # getting a templated query function for multiple execution\n",
    "        selectTemplate = self.templatedSelectQuery(\"select %s from METADATA where SYMBOL_CODE='%s' and DATASOURCE='%s';\")\n",
    "        columnsToQuery1 = ['INTERVAL_START_TS', 'INTERVAL_END_TS']\n",
    "        columnsToQuery2 = ['FILE_NAME']\n",
    "        \n",
    "        # dictionaries to return\n",
    "        returnMultiIndex = {}\n",
    "        returnTupleDict0 = {}\n",
    "        returnMultiDict1 = {}\n",
    "        returnMultiDict2 = {}\n",
    "        \n",
    "        # determines the unseen intervals for (symbol, adapter) pairs and downloads the data, then constructs the \n",
    "        # return object\n",
    "        for symbol in symbolOrSymbolList:\n",
    "            for adapter in downloadAdapterList:\n",
    "                \n",
    "                # queries the database for already stored CSV files what are matching to the given (symbol, adapter) pair\n",
    "                timestampRecords = selectTemplate(columnsToQuery1, (symbol, adapter.datasourceName))\n",
    "                \n",
    "                # creates inputs for interval calculation and determines the missing intervals\n",
    "                alreadyStoredIntervals = [(Utils.convertUnixTimeStampToDateTime(x[0]), \\\n",
    "                                           Utils.convertUnixTimeStampToDateTime(x[1])) \\\n",
    "                                          for x in timestampRecords]\n",
    "                unknownIntervalsToDownload = self.__calculateIntervalsToDownload(interval, alreadyStoredIntervals)\n",
    "                \n",
    "                # if necessary, downloads the missing intervals\n",
    "                if (unknownIntervalsToDownload != None and len(unknownIntervalsToDownload) > 0):\n",
    "                    for intervalToDownload in unknownIntervalsToDownload:\n",
    "                        self.__downloadSymbolData([symbol], [adapter], \\\n",
    "                                                  intervalToDownload[0], intervalToDownload[1], maxRetryCnt)\n",
    "                \n",
    "                # requery the database for the full (possibly some new) set of matching data\n",
    "                csvPathRecords = selectTemplate(columnsToQuery2, (symbol, adapter.datasourceName))\n",
    "                \n",
    "                # generate a single dataframe of all relevant data regarding to the given symbol and datasource\n",
    "                csvPathList = [x[0] for x in csvPathRecords]\n",
    "                jdf = self.__readMultipleCSVsIntoSingleDataframe(csvPathList)\n",
    "                \n",
    "                # cleaning and filtering data\n",
    "                InputPreprocessor.cleanDataframe(jdf)\n",
    "                self.__dropIntervalInplace(jdf, interval[0], interval[1], startInclusiveFilter, endInclusiveFilter)\n",
    "                InputPreprocessor.cleanDataframe(jdf)\n",
    "                \n",
    "                # returns the dataframes addressed by four dictionaries for convenient handling\n",
    "                returnMultiDict1.setdefault(symbol, {})[adapter.datasourceName] = jdf\n",
    "                returnMultiDict2.setdefault(adapter.datasourceName, {})[symbol] = jdf\n",
    "                returnTupleDict0[(symbol, adapter.datasourceName)] = jdf\n",
    "                returnMultiIndex.setdefault(symbol, {})[adapter.datasourceName] = jdf\n",
    "                returnMultiIndex.setdefault(adapter.datasourceName, {})[symbol] = jdf\n",
    "                returnMultiIndex[(symbol, adapter.datasourceName)] = jdf\n",
    "                \n",
    "        return returnMultiIndex, returnTupleDict0, returnMultiDict1, returnMultiDict2\n",
    "    \n",
    "    \n",
    "    \n",
    "    def getConsolidatedData(self, symbolOrSymbolList, interval, maxRetryCnt = 10, intervalExtension = 10):\n",
    "        \"\"\" Returns the colsolidated OHLCV data of both datasources\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        deltatime = timedelta(days = abs(intervalExtension))\n",
    "        extendedInterval = (interval[0]-deltatime, interval[1]+deltatime)\n",
    "        \n",
    "        self.logger.info(\"getting consolidated data: \" \\\n",
    "                         + str(symbolOrSymbolList) + \"\\t\" + str(interval) + \"\\t\" + str(intervalExtension))\n",
    "        \n",
    "        gfda = GoogleFinanceDownloadAdapter()\n",
    "        yfda = YahooFinanceDownloadAdapter()\n",
    "        \n",
    "        _, _, md1, _ = self.getData(symbolOrSymbolList, extendedInterval, \\\n",
    "                                         downloadAdapterList=[gfda, yfda], maxRetryCnt=maxRetryCnt)\n",
    "        \n",
    "        # dictionaries to return\n",
    "        returnMultiIndex = {}\n",
    "        returnTupleDict0 = {}\n",
    "        returnMultiDict1 = {}\n",
    "        returnMultiDict2 = {}\n",
    "        \n",
    "        # determines the unseen intervals for (symbol, adapter) pairs and downloads the data, then constructs the \n",
    "        # return object\n",
    "        for symbol in md1.keys():\n",
    "            \n",
    "            keys = list(md1[symbol].keys())\n",
    "            df1 = md1[symbol][keys[0]]\n",
    "            df2 = md1[symbol][keys[1]]\n",
    "            \n",
    "            print(symbol, keys)\n",
    "            print(df1)\n",
    "            print(df2)\n",
    "            \n",
    "            \n",
    "        \n",
    "        return None, None, None, None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the downloading of all records only one time is ensured, it can be seen that the multiple download requests can cause fragmentation on data and metadata, which is hidden from the user. The fragmented data and metadata can be compacted into coherent segments, but solving this problem was not in the scope of this task.\n",
    "<br />\n",
    "Further option to improve the DataManager is to store SP400 symbols in a database and update from Wikipedia only if needed. This solution is out of the current scope too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interval border constants\n",
    "DATA_START          = datetime(2016,1,1)\n",
    "DATA_END            = datetime(2017,1,1)\n",
    "VARIANCE_START      = datetime(2016,2,11)\n",
    "VARIANCE_END        = datetime(2016,11,8)\n",
    "MINMAX_START        = datetime(2016,1,18)    # including\n",
    "MINMAX_END          = datetime(2016,10,18)   # excluding\n",
    "STD_START           = datetime(2016,4,17)\n",
    "STD_END             = datetime(2016,12,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfSymbols = getSP400ListFromWikipedia()\n",
    "truncList = listOfSymbols[3:6]\n",
    "\n",
    "gfda = GoogleFinanceDownloadAdapter()\n",
    "yfda = YahooFinanceDownloadAdapter()\n",
    "\n",
    "dm = DataManager([gfda, yfda])\n",
    "_, _, _, multiDict2 = dm.getData(truncList, (DATA_START, DATA_END))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating data fringerprints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to calculate the data fingerprints.<br/>\n",
    "- A solution can be iterating over the datasources, then iterating over the possible days and query the symbol tables for matching records and calculate the (5 dimensional) value vector (the __Open__, __High__, __Low__, __Close__, __Volume__ aggregates), then save each calculated value into a new dataframe/CSV file.\n",
    "- Another way to do is to append after each other the symbol tables, add new columns to the new table which are indicating the __Symbol__ and __Datasource__ values, then calculate the aggregations with a __group-by(Datasource, Symbol, Date)__ grouping.\n",
    "\n",
    "Since the tables are containing very few records (for a year), the sizes of the CSV files are typically less than 100 kbytes, storing approximately 2x400x100 kbytes in memory should not be a problem for an avarage personal computer. The problem does not require big data computation architecture or algorithms, that's why a mixture of the above solutions was selected for impelemtation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# function for adding a new column to a dataframe\n",
    "def addColumnToDataframe(dataframe, columnName, columnContent):\n",
    "    dataframe[columnName] = columnContent\n",
    "\n",
    "# generates a new column into the dataframe with symbol's name\n",
    "def fillDataframeWithSymbolName(dataframe, symbol):\n",
    "    addColumnToDataframe(dataframe, \"Symbol\", np.full(dataframe.shape[0], symbol, dtype=object))\n",
    "\n",
    "\n",
    "# getting data for fingerprint calculation by using the intervals\n",
    "_, _, _, dataSetForMeanCalculation   = dm.getData(truncList, (DATA_START, DATA_END))\n",
    "_, _, _, dataSetForVarCalculation    = dm.getData(truncList, (VARIANCE_START, VARIANCE_END))\n",
    "_, _, _, dataSetForMinMaxCalculation = dm.getData(truncList, (MINMAX_START, MINMAX_END),     endInclusiveFilter=True)\n",
    "_, _, _, dataSetForStdCalculation    = dm.getData(truncList, (STD_START, STD_END),           endInclusiveFilter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decorator for executing the same pre and post computation for each calculation type\n",
    "def prepareDataAndProcessResults(fileNamePart):\n",
    "    \n",
    "    def decorator_func(calculationToExecute):\n",
    "        \n",
    "        def wrapper_func(*args, **kwargs):\n",
    "\n",
    "            # abbreviating the variable name \n",
    "            datasource = args[0]\n",
    "            df = args[1][datasource]\n",
    "\n",
    "            # getting symbols from the dataframes\n",
    "            symbols = list(df.keys())\n",
    "            \n",
    "            # adding 'symbol' column to each dataframe\n",
    "            [fillDataframeWithSymbolName(df[symbol], symbol) for symbol in symbols]\n",
    "\n",
    "            # creating and reindexing concatenated dataframe\n",
    "            dataframe = pd.concat([df[symbol] for symbol in symbols])\n",
    "            InputPreprocessor.cleanDataframe(dataframe, uniqueColNameList = [\"Date\", \"Symbol\"])\n",
    "            \n",
    "            # calculating the result table\n",
    "            retDf = calculationToExecute(dataframe)\n",
    "            \n",
    "            # write the results into CSV files\n",
    "            fileName = \"%s__%s.csv\" % (datasource, fileNamePart)\n",
    "            filePath = os.path.join(FINGERPRINTS_DIRECTORY_NAME, fileName)\n",
    "            retDf.to_csv(filePath, sep=',', encoding='utf-8')\n",
    "            \n",
    "            with pd.option_context('display.max_rows', None):\n",
    "                print(fileName)\n",
    "                display(retDf)\n",
    "            \n",
    "        return wrapper_func\n",
    "    \n",
    "    return decorator_func\n",
    "\n",
    "\n",
    "# defining calculation types\n",
    "\n",
    "@prepareDataAndProcessResults(fileNamePart = \"mean\")\n",
    "def calculateMean(dataframe):\n",
    "    return dataframe.groupby( [ \"Date\"] ).mean()\n",
    "\n",
    "\n",
    "@prepareDataAndProcessResults(fileNamePart = \"std\")\n",
    "def calculateStd(dataframe):\n",
    "    return dataframe.groupby( [ \"Date\"] ).std()\n",
    "\n",
    "\n",
    "@prepareDataAndProcessResults(fileNamePart = \"lowest_highest\")\n",
    "def calculateMinMax(dataframe):\n",
    "    df = dataframe.groupby( [ \"Symbol\"] )[\"Close\"].agg(['min', 'max'])\n",
    "    df.columns = [\"lowest_close\", \"highest_close\"]\n",
    "    df.index.name = \"ticker\"\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "@prepareDataAndProcessResults(fileNamePart = \"var\")\n",
    "def calculateVar(dataframe):\n",
    "    return dataframe.groupby( [ \"Date\"] ).var()\n",
    "\n",
    "\n",
    "# iterating over the datasets returned by the data layer\n",
    "def calculateFingerprints(dict_dataset, calculateFingerprint):\n",
    "\n",
    "    # iterating over datasources\n",
    "    for datasource in dict_dataset.keys():\n",
    "        \n",
    "        calculateFingerprint(datasource, dict_dataset)\n",
    "\n",
    "\n",
    "# calculating Mean, Variance values and Min/Max prices over all ticker symbol for each day and some or all data fields \n",
    "calculateFingerprints(dataSetForMeanCalculation,    calculateMean)\n",
    "calculateFingerprints(dataSetForStdCalculation,     calculateStd)\n",
    "calculateFingerprints(dataSetForMinMaxCalculation,  calculateMinMax)\n",
    "calculateFingerprints(dataSetForVarCalculation,     calculateVar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the solution above an iteration went through the datasources where a new column with the symbol names was added to each dataframe.<br />\n",
    "Four aggregation methods were defined (for calculating __mean__, __std__, __variance__ and __minmax__ values) which were executed with python's decorator pattern, where auxilary operations were executed before and after the concrete calculation. These supplementary operations were e.g.: creating a big, concatenated table, cleaning the concatenated table by using two keys (_Date_ and _Symbol_) and finally writing the results into dataframe after the computation was executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point each pandas dataframe (for computing datasource difference) is already extended with an extra column containing symbol values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeStdOfDatasourceDifference(df1, df2, fileName):\n",
    "    \"\"\" This function calculates the difference between the datasources \"\"\"\n",
    "    \n",
    "    # getting the dataframes by their keys from the dictionary\n",
    "    concatenated_dataframe_google = pd.concat([df1[symbol] for symbol in df1.keys()])\n",
    "    concatenated_dataframe_yahoo  = pd.concat([df2[symbol] for symbol in df2.keys()])\n",
    "    \n",
    "    # cleaning data\n",
    "    InputPreprocessor.cleanDataframe(concatenated_dataframe_google, uniqueColNameList = [\"Date\", \"Symbol\"])\n",
    "    InputPreprocessor.cleanDataframe(concatenated_dataframe_yahoo,  uniqueColNameList = [\"Date\", \"Symbol\"])\n",
    "    \n",
    "    # complex key creation for table subtraction\n",
    "    concatenated_dataframe_google.set_index(['Date', 'Symbol'], inplace=True)\n",
    "    concatenated_dataframe_yahoo.set_index(['Date', 'Symbol'], inplace=True)\n",
    "    \n",
    "    # compute the subtracted table\n",
    "    subtracted = concatenated_dataframe_google.subtract(concatenated_dataframe_yahoo)\n",
    "    \n",
    "    # compute the STD over the subtracted table\n",
    "    retDf = subtracted.groupby( [ \"Date\"] ).std()\n",
    "    \n",
    "    # write the results into CSV files\n",
    "    filePath = os.path.join(FINGERPRINTS_DIRECTORY_NAME, fileName)\n",
    "    retDf.to_csv(filePath, sep=',', encoding='utf-8')\n",
    "    \n",
    "    with pd.option_context('display.max_rows', None):\n",
    "        print(fileName)\n",
    "        display(retDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the deviation of the difference between the datasources\n",
    "computeStdOfDatasourceDifference(dataSetForStdCalculation['google'], \\\n",
    "                                 dataSetForStdCalculation['yahoo'], \\\n",
    "                                 'google_yahoo__comparison.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above solution two big tables were generated containing every OHLCV data of the two datasources which were cleaned, reindexed and identified duplicates were filtered out. Duplicates were identified by the key set: _Date_ and _Symbol_.<br />\n",
    "A big table with the element wise subtraction of the two original tables was calculated (where the keys stayed the original _Data_ and _Symbol_).<br />On this subtracted table an aggregation operation (an __std__ calculation with a __group-by__) was applied by the _Date_ column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to consolidate the data of the two different datasources, but in almost every solution this process consists of these sub steps:\n",
    "- Identify missing values\n",
    "- Identify outliers\n",
    "- Calculate approximations for consolidated data (while considering the missing and outlier values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervalExtension = 5\n",
    "dm.getConsolidatedData(truncList, (DATA_START, DATA_END), intervalExtension = intervalExtension)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implemented solution does the following:\n",
    "- Flags the outlier records by using the __3-sigma rule__ on the pricing and volume data\n",
    "  - Since the pricing data in each column of the dataframe originates from a time series by aggregation operators, it's logical to extract and concatenate the values from all ___pricing data columns___ and the ___volume___ column, then apply a 1-dimensional filtering on them, instead of applying 1-dimensional filtering on each (_Open_, _High_, _Low_, _Close_) column\n",
    "    - Using a filtering method for 2 dimensional space (over the _pricing_ and _volume_ data) probably would be a better but more complex solution here\n",
    "  - Any other outlier detection mechanism could be used here instead of the 3-sigma rule. Probably a more effective solution would be using here a clustering algorithm in 2 dimensional space (_pricing data_ and _volume data_), but that would require much more experimentation and it's probably not in the scope ot this demonstrative application\n",
    "- Creates a merged table by both datasources for the given symbol, which are containing every pricing and volume data columns of the original tables indexed by _Date_\n",
    "- Uses __weighted K-Nearest Neighbors regression__ algorithm to approximate the real value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataSetForMinMaxCalculation['google']['ACIW']\n",
    "r, f, o = InputPreprocessor.filterOutliers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
